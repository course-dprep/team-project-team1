---
title: "deliverable_week2"
output:
  pdf_document: default
  html_document: default
---

# A topic modeling study: The association between Yelp review sentiment and restaurant survival or closure

*Which aspects of the feedback provided by Yelp customer reviews are associated with restaurant closures?* 

## Motivation

Online customer reviews play a significant role in shaping how restaurants are perceived by consumers. When searching for new places to eat at, consumers may resort to customer review platforms, such as Yelp, to assess which would be worth giving a try. Therefore, the valence of a restaurant's reviews on several different aspects can impact its success, and ultimately, its survival. Restaurant closures are the endmost sign that a restaurant was unsuccessful as a business, which can happen for several reasons. The aim of this project is then to establish which, if any, aspects of customer reviews are associated to restaurant closures.  

## Managerial benefits

- Early risk detection: By monitoring review sentiment and topics, managers can identify warning signals of potential closure risk before financial issues become critical.

- Improved customer experience: Insights into recurring complaints or praised aspects help managers focus resources on what customers value most.

- Resource prioritization: Helps allocate marketing and operational investments toward areas with the greatest impact on survival.

## Data

To succeed in our purpose the following datasets were used:

- **yelp_academic_dataset_business**, which contains business information and open/closed status,
- **yelp_academic_dataset_review**, which contains review texts and metadata
- **yelp_academic_dataset_checkin**, which contains information regarding the last check-ins

From these sources we constructed a balanced sampled dataset, **reviews_sampled.rds**.

It includes 100 restaurants (50 open and 50 closed) with at least 100 reviews since 2018.

For each restaurant, 50 reviews were randomly selected, resulting in about 5,000 observations with 11 variables covering text, dates, and business identifiers.

For closed restaurants, only reviews up to the date of the last check-in (i.e., while they were still active) are considered.

The **table** below summarizes the most important variables at this stage of the project: 

```{r}
# Create a table with the variable names

variable_table <- data.frame(
  Variable = c("business_id", "review_id", "text", "stars", "date", "user_id", "is_open", "checkin", "last_checkin"),
  Description = c("The business ID of the reviewed company", "The ID of the review", "The complete review of the user", "The amount of stars (between 1-5) given by the user", "The timestamp of the review", "The ID of the user who submitted the review", "Whether the restaurant is active/open (1) or closed (0)", "All recorded checkin timestamps of reviews for a company", "The last recorded checkin timestamp of a review for a company")
)

variable_table
```

## Method

To succeed in our purpose the following methods will be used: 
- **Sentiment analysis** to classify reviews (positive vs negative)
- **Topic modeling** to understand the relevant topics in positive vs negative reviews
- **Regression** to test whether certain topics associated with a specific type of review (positive or negative)  impacts on restaurants’ closure

This integrated approach provides a clear and data-driven way to link review content to business outcomes.

## Data downloading

```{r}
#Check if all dependencies are installed, and install if needed
required_packages <- c("tidyverse", "googledrive", "data.table", "here")

for (pkg in required_packages) {
	if (!requireNamespace(pkg, quietly = TRUE)) {
		suppressMessages(install.packages(pkg))
	}
}
#Load all the dependencies
invisible(lapply(required_packages, function(pkg) {
	suppressPackageStartupMessages(library(pkg, character.only = TRUE))
}))


#Check if TinyTex is installed
if (!tinytex::is_tinytex()) {
	message("TinyTeX not installed. Please run tinytex::install_tinytex() in the console.")}

#Always ensures data/raw_data/ exists at the project root
if (!dir.exists(here("data", "raw_data"))) {
	dir.create(here("data", "raw_data"), recursive = TRUE)
}

#Datasets + Drive listing
datasets <- c(business="business", checkin="checkin", tip="tip", user="user", review="review")
googledrive::drive_deauth()
folder_id <- "1WHSh8ZQYzQ3IQI8tJX90cYGR4bDy13v3"
drive_folder <- drive_ls(as_id(folder_id))

#Loop: ensure RDS exists; if not, make it (download CSV if needed); never assign to env
for (dataset in datasets) {
	rds_path <- here("data", "raw_data", paste0(dataset, ".rds"))
	csv_path <- here("data", "raw_data", paste0("yelp_academic_dataset_", dataset, ".csv"))
	
	if (file.exists(rds_path)) {
		message("OK: ", rds_path, " already exists. Skipping.")
		next
	}
	
	if (!file.exists(csv_path)) {
		message("CSV missing for ", dataset, ". Downloading from Drive...")
		file <- drive_folder[str_detect(drive_folder$name, dataset), ]
		size_bytes <- as.numeric(file$drive_resource[[1]]$size)
		size_mb <- round(size_bytes / (1024^2), 2)
		message("Download size: ", size_mb, " MB")
		googledrive::drive_download(as_id(file$id), path = csv_path, overwrite = TRUE)
		rm(file)
	} else {
		message("Found CSV: ", csv_path)
	}
	
	message("Reading CSV with fread() → writing RDS: ", rds_path)
	t0 <- Sys.time()
	dat <- data.table::fread(csv_path, showProgress = TRUE)
	saveRDS(dat, rds_path, compress = FALSE)              # consider:  compress = FALSE for speed
	rm(dat); gc()
	message("Done (", round(difftime(Sys.time(), t0, units = "secs"), 1), " sec).")
}

```

## Data preparation

### Sampling

#### Loading the raw data

For the sampling, the following datasets are needed:
- review.rds
- business.rds

```{r}
#loading data.tables into environment
review <- readRDS(here("data", "raw_data","review.rds"))
business <- readRDS(here("data", "raw_data","business.rds"))

#collect $business_id vector for all Restaurant businesses
restaurant_ids <- business[
  grepl("Restaurants", categories, fixed = TRUE), 
  unique(business_id)
]

#set index to speed up the data join
setindex(review, business_id)

#data join
review_restaurants <- review[.(restaurant_ids), on = "business_id", nomatch = 0L]

#check
uniqueN(review_restaurants$business_id)  # should return 52268
```

#### Counting n. of reviews per Restaurant

To make sure that we have enough training data for our prediction model, a restaurant needs to have at least 50 reviews in the dataset. This makes it a necessity to compute the # of reviews per restaurant in the data.

```{r}
#stores data.table that includes business_id and corresponding number of reviews
review_counts <- review_restaurants[, .(n_reviews = .N), by = business_id]
```

#### Collecting open/closed variable

Since our research is also focused on threat detection, it is important to check what percentage of restaurants is actually closed in the dataset. If this percentage is very low, we need to inflate the percentage of closed restaurants in our dataset as this will improve the model's ability to identify at-risk businesses due to the increased representation of the minority class. Without rebalancing, the model could become very "good" by predicting all businesses remain open, while completely failing to capture characteristics that could predict risk of closing.

```{r}
restaurant_status <- business[.(restaurant_ids), on = "business_id", .(business_id, is_open)]

```

The open vs. closed ratio of restaurants in the Yelp dataset is:

**`r round(mean(restaurant_status$is_open) * 100, 1)`%** open and 
**`r round((1 - mean(restaurant_status$is_open)) * 100, 1)`%** closed.

```{r}
# merge counts with open/closed status
review_counts_status <- review_counts[
  business[, .(business_id, is_open)], 
  on = "business_id"
]

# how many restaurants are closed and have >= 50 reviews?
closed_50plus <- review_counts_status[n_reviews >= 50 & is_open == 0, .N]
closed_50plus   #no of restaurants that are closed but have 50+ reviews
```

#### Creating business_id vectors

In this part, we will create business_id vectors with the following characteristics:

**closed_ids vector:**
- Type = Restaurant
- Restaurant is currently closed
- No. of reviews since 01 / 01 / 2018 is #100 or higher

**open_ids vector:**
- Type = Restaurant
- Restaurant is currently open
- No. of reviews since 01 / 01 / 2018 is #100 or higher

```{r}
checkin <- readRDS(here("data", "raw_data", "checkin.rds"))	#get checkin data in environment

checkin[,date :=na_if(str_trim(date), "")]

checkin[,last_checkin := as.IDate(fifelse(is.na(date),NA_character_, str_sub(date, -19,-10)))]

is.na(checkin$last_checkin)%>%table() #no NA's, so extra safe for future operations


```


```{r}
date_cutoff <- "2018-01-01"

    #CLOSED RESTAURANTS

closed_ids <- restaurant_status[is_open == 0, unique(business_id)]  #closed restaurants id's

since2018_closed <- review_restaurants[
  checkin[, .(business_id, last_checkin)],          # bring in last_checkin by business
  on = "business_id"
][
  business_id %chin% closed_ids & 
  date >= as.IDate(date_cutoff) & 
  !is.na(last_checkin) & 
  date <= last_checkin,                             # only count reviews before last recorded check in time
  .N,
  by = business_id
]

since2018_closed[N >= 100, .N]  #no. of closed restaurants that have 100+ reviews since 2018, before their last checkin

closed_100plus_since18 <- since2018_closed[N >= 100, business_id] #business_id vector

    #OPEN RESTAURANTS

open_ids <- restaurant_status[is_open == 1, unique(business_id)]  ##open restaurants id's

since2018_open <- review_restaurants[
  business_id %chin% open_ids & date >= as.IDate(date_cutoff),
  .N,
  by = business_id
]   #list of restaurants that are open and their no. of reviews since 01 / 01 / 2018

since2018_open[N >= 100, .N]  #no. of open restaurants that have 100+ reviews since 2018 

open_100plus_since18 <- since2018_open[N >= 100, business_id] #business_id vector

```

#### Randomly selecting business_id's from the business_id vectors

We just created the following two business_id vectors:

- **closed_100plus_since18**
- **open_100plus_since18**

As discussed earlier, we will inflate the no. of closed businesses to improve the models ability to identify at-risk businesses. The percentage of closed businesses in our training dataset will be 50%.

```{r}
set.seed(2310)    #setting seed for reproducibility

sampled_closed_ids <- sample(closed_100plus_since18, 50)  #sampled business id's for closed restaurants

sampled_open_ids <- sample(open_100plus_since18, 50)      #sampled business id's for open restaurants

```

#### Randomly selecting reviews for the selected restaurants

For the randomly selected business id's, we need to select a certain no. of reviews as well. We opted for 50 reviews per restaurant. This means our final training dataset will contain:

( 50 closed restaurants * 50 reviews ) + ( 50 open restaurants * 50 reviews ) = #5000 reviews.

```{r}
sampled_ids <- c(sampled_closed_ids, sampled_open_ids)  #vector of all the sampled id's

review_restaurants[, date_id := as.IDate(date)] #creating date id (YYYY - MM - DD)

# copy last_checkin from `checkin` into `review_restaurants` by business_id
review_restaurants[checkin, on = "business_id", last_checkin := i.last_checkin]

review_restaurants_since18 <- review_restaurants[
  date_id >= as.IDate(date_cutoff) &	# after 2018
  date_id <= last_checkin				# before restaurant's last checkin
]

setindex(review_restaurants_since18, business_id) #improves speed 

set.seed(2310)  #set seed for reproducibility

reviews_sampled <- review_restaurants_since18[
  business_id %chin% sampled_ids,   #select only rows where business_id matches with samples_ids
  .SD[sample(.N, 50)],              #sample 50 out of total .N ->
  by = business_id                  #per business_id
]

```

#### Storing the training data for future use

We now store the selected traning data for future use in a training_data folder
- Check if training_data folder is present
- Create if needed
- Store a .rds file of the training data in that folder.

```{r}
# Ensure training_data directory
dir.create(here("data", "training_data"), recursive = TRUE, showWarnings = FALSE)

# Store the training data as .rds file
saveRDS(reviews_sampled, here("data","training_data","reviews_sampled.rds"))
local_path <- here("data","training_data","reviews_sampled.rds")	#local path where file is stored
file_name <- "reviews_sampled.rds"

folder_id <- "1oRNbZpA4kXZRsvcNe5K1FRYFKqqT5W2h"
			
```

##### Alternative

*the ultimate dataset can be downloaded by google drive*

```{r}
folder_id <- "1oRNbZpA4kXZRsvcNe5K1FRYFKqqT5W2h"	#folder id
googledrive::drive_deauth()
folder <- drive_ls(as_id(folder_id))
filename <- "reviews_sampled.rds"
file_id <- folder[folder$name==filename] %>% pull(id) %>% as.character()
drive_download(as_id(file_id), path = here("data", "training_data",filename), overwrite = TRUE)

reviews_sampled <- readRDS(here("data", "training_data", filename))
```




